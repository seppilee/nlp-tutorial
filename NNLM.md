 ### 기존 N-gram 언어 모델의 한계
 - N이 커지면 커질수록 계산 복잡도가 기하급수적으로 증가한다.
 - 데이터에서 관측하지 못한 언어데 대해 희소문제를 풀 수가 없다. 
 
 언어 모델 또한 단어의 유사도를 학습할 수 있도록 설계한다면, 훈련 
 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있을 겁니다. 
 그리고 이런 아이디어를 가지고 탄생한 언어 모델이 신경망 언어 모델 NNLM입니다.
 
 
 ![NNLM](https://wikidocs.net/images/page/45609/nnlm5_final.PNG)
 
 
 
NNLM의 이점과 한계
 -이점
 모든 n-gram을 저장하지 않아도 된다는 점에서 n-gram 언어 모델보다 저장 공간의 이점을 가집니다.
 -한계
 고정된 길이의 입력(Fixed-length input)
 
 
